{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index import (LLMPredictor, SimpleDirectoryReader, ServiceContext)\n",
    "from langchain.llms import AzureOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "from llama_index.indices.prompt_helper import PromptHelper\n",
    "\n",
    "os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
    "os.environ[\"OPENAI_API_VERSION\"]=\"2023-05-15\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://devcommunity.openai.azure.com/\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"ece805490dd74e038e959f3f5e3908dc\"\n",
    "\n",
    "# Create an instance of Azure OpenAI\n",
    "# Replace the deployment name with your own\n",
    "llm = AzureOpenAI(\n",
    "    deployment_name=\"azure_davinci\",\n",
    "    model_name=\"text-davinci-003\",\n",
    "    temperature=0,\n",
    "    max_tokens=2000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Requests to the Get a vector representation of a given input that can be easily consumed by machine learning models and algorithms. Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 1 second. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit..\n"
     ]
    }
   ],
   "source": [
    "llm_predictor = LLMPredictor(llm=llm)\n",
    "embedding_llm = LangchainEmbedding(OpenAIEmbeddings(), embed_batch_size=1)\n",
    "\n",
    "max_input_size = 1000\n",
    "num_output = 256\n",
    "chunk_size_limit = 1000\n",
    "max_chunk_overlap = 20\n",
    "\n",
    "prompt_helper = PromptHelper(context_window=max_input_size,num_output=num_output, chunk_size_limit=chunk_size_limit)\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, embed_model=embedding_llm, prompt_helper=prompt_helper)\n",
    "\n",
    "\n",
    "from llama_index.indices.vector_store.base import VectorStoreIndex\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders import PyMuPDFLoader, WebBaseLoader\n",
    "\n",
    "# pdf_folder_path = \"C:/Code/Graph API Doc/\"\n",
    "# pdf_file_path = \"C:/Code/Graph API Doc/graph-api-graph-rest-1.0.pdf\"\n",
    "docs_path = \"./data\"\n",
    "\n",
    "documents = SimpleDirectoryReader(docs_path, required_exts=\".md\").load_data()\n",
    "\n",
    "#pdfData = PyMuPDFLoader(pdf_file_path).load()\n",
    "# loaders = [PyMuPDFLoader(os.path.join(pdf_folder_path, fn)) for fn in os.listdir(pdf_folder_path)]\n",
    "#text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 0)\n",
    "#all_splits = text_splitter.split_documents(pdfData)\n",
    "#try:\n",
    "index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "#except:\n",
    "#    print(\"Something went wrong\")\n",
    "\n",
    "#index = VectorstoreIndexCreator().from_loaders(loaders=loaders)\n",
    "\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: MS Graph API Endpoint to create a team with request payload?\n",
      "Completion: \n",
      "The MS Graph API endpoint to create a team with request payload is:\n",
      "\n",
      "POST https://graph.microsoft.com/beta/teams\n",
      "Content-Type: application/json\n",
      "\n",
      "{\n",
      "  \"template@odata.bind\": \"https://graph.microsoft.com/beta/teamsTemplates('standard')\",\n",
      "  \"displayName\": \"My Sample Team\",\n",
      "  \"description\": \"My Sample Teamâ€™s Description\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "template = '''Answer the given prompt with MS Graph API endpoint. The answer should be strictly in following form:\n",
    "HTTP METHOD \n",
    "\n",
    "HTTP ENDPOINT URL\n",
    "\n",
    "HTTP REQUEST PAYLOAD\n",
    "{}'''\n",
    "\n",
    "# prompt = \"MS Graph API Endpoint to get my user profile?\"\n",
    "# prompt = \"MS Graph API Endpoint to select displayName and userPrincipalName from my user profile?\"\n",
    "# prompt = \"MS Graph API Endpoint to get the teams I have joined?\"\n",
    "# prompt = \"MS Graph API Endpoint to select the displayname of the teams I have joined?\"\n",
    "prompt = \"MS Graph API Endpoint to create a team with request payload?\"\n",
    "# prompt = \"MS Graph API Endpoint to list my chats?\"\n",
    "# prompt = \"MS Graph API Endpoint to list my top 2 chats?\"\n",
    "# prompt = \"MS Graph API Endpoint to create a message in the chatId 19:meeting_YWI4MWUxM2ItYzMyNC00ZWFmLWIxNzYtYzE2YjQxMjQwZGM1@thread.v2 with request body\"\n",
    "\n",
    "query = template.format(prompt)\n",
    "\n",
    "response = query_engine.query(prompt)\n",
    "\n",
    "print(f\"Question: {prompt}\")\n",
    "\n",
    "print(f\"Completion: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202 Accepted /teams('eda6b64e-5417-49d5-8d8d-e65cd59c15f2')/operations('b3a1a049-e9e2-4ad4-8a2a-a60ec4fb60ca')\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import re\n",
    "\n",
    "responseArr = response.response.split()\n",
    "headers = {\"Authorization\": \"Bearer eyJ0eXAiOiJKV1QiLCJub25jZSI6ImxQMnVqMnJwcW1PbEhfWDVpTVFlUUV5cmU5OGp1Q0VJZUlIZ2Q5QTlvLVkiLCJhbGciOiJSUzI1NiIsIng1dCI6Ii1LSTNROW5OUjdiUm9meG1lWm9YcWJIWkdldyIsImtpZCI6Ii1LSTNROW5OUjdiUm9meG1lWm9YcWJIWkdldyJ9.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.Eieip_gP93xcC01Z2toMWqEQWB_sjPBYnf5LJ0ewGfqwUsdNwlnCeM7szDD5zESLZs5U9iFTB9B08upEE_HGko2QQnnQt9QZiCmIoq-JbffM0ih5AnoFRXM6HYAdCOkL62nOZOCq4nDV6F0Z3bP7ivQpJ6cvcaX39wgctAR_8guGyPjEX3_rucSLgSaoT_ZIJ54T7aH7G3NU4x9BEBxjaaUP0iTCk2CdZcEEIOME_hCEH4HK4rgESP7WSzbZu2zdEV6Oqw6kD-gmjQK3Mnnf1KXeA1M7sqRBxJ9rinA9zmemaxYU2VZNfTgXK_MiabS6kbPLoNU5yrXdbKOPnu7sTw\"}\n",
    "\n",
    "idx = 0\n",
    "httpMethodFound = False\n",
    "\n",
    "for responseStr in responseArr:\n",
    "    if (responseStr in [\"GET\", \"POST\", \"PUT\", \"DELETE\"]):\n",
    "        httpMethodFound = True\n",
    "        break\n",
    "    else:\n",
    "        idx = idx + 1\n",
    "\n",
    "if httpMethodFound:\n",
    "    if responseArr[idx] == \"GET\":\n",
    "        apiresponse = requests.get(responseArr[idx + 1], headers=headers)\n",
    "        json_api_response = apiresponse.json()\n",
    "        parsed = json.dumps(json_api_response, indent=4)\n",
    "        print(parsed)\n",
    "    elif responseArr[idx] == \"POST\":\n",
    "        responseStr = response.response \n",
    "        startIndex = responseStr.find('{')\n",
    "        requestBody = responseStr[startIndex:]\n",
    "        jsonBody = json.loads(requestBody)\n",
    "        apiresponse = requests.post(responseArr[idx + 1], headers=headers, json=jsonBody)\n",
    "        print(f\"{apiresponse.status_code} {apiresponse.reason} {apiresponse.headers['Location']}\")\n",
    "    else:\n",
    "        print(\"Method not supported\")\n",
    "else:\n",
    "    print(\"Sorry, was not able to parse the response properly from the response prompt! Please try asking the question differently.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
